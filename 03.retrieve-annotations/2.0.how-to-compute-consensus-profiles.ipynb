{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "from copairs.map import mean_average_precision, multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I want to compare six different scenarios for either computing the consensus ORF profile or how to handle duplicate profiles for downstream analysis in copairs and identify the best scenario or the one that makes the most sense. For doing this, I will be using two sets of profiles, `ORF` and `ORF-CRISPR-pipeline` and two sets of annotations, CORUM complex ID and HGNC gene group ID. Here are the six scenarios:\n",
    "- s1: group profiles by JCPID\n",
    "- s2: group profiles by GeneID\n",
    "- s3: group profiles by JCPID but use gene ID as pos_diffby in copairs\n",
    "- s4: Keep the most replicable reagent when there are many targeting the same gene\n",
    "- s5: Keep a random ORF reagent\n",
    "- s6: Keep the reagent with the longest insert and highest number of proteins matching and then randomly choose one reagent if there are multiple ones with the same number of protein matching and the same insert length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ID_col = \"Metadata_NCBI_Gene_ID\"\n",
    "reagent_ID_col = \"Metadata_JCP2022\"\n",
    "\n",
    "scenarios = {\n",
    "    \"s1\": f\"{reagent_ID_col}\",\n",
    "    \"s2\": f\"{gene_ID_col}\",\n",
    "    \"s3\": f\"{reagent_ID_col}\",\n",
    "    \"s4\": f\"{reagent_ID_col}\",\n",
    "    \"s5\": f\"{reagent_ID_col}\",\n",
    "    \"s6\": f\"{reagent_ID_col}\",\n",
    "}\n",
    "\n",
    "annotations = {\n",
    "    \"corum\": {\n",
    "        \"annotation_col\": \"Metadata_complexname\",\n",
    "        \"multi_label_col\": \"Metadata_corum_complex_list\",\n",
    "    },\n",
    "    \"gene-group\": {\n",
    "        \"annotation_col\": \"Metadata_gene_group_id\",\n",
    "        \"multi_label_col\": \"Metadata_gene_group_list\",\n",
    "    },\n",
    "}\n",
    "\n",
    "profiles = {\n",
    "    \"ORF\": \"wellpos_cc_var_mad_outlier_featselect_sphering_harmony\",\n",
    "    \"ORF-CRISPR-pipeline\": \"wellpos_var_mad_int_featselect_harmony_PCA\",\n",
    "}\n",
    "\n",
    "batch_size = 20000\n",
    "null_size = 20000\n",
    "fdr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df = pd.DataFrame()\n",
    "map_df = pd.DataFrame()\n",
    "\n",
    "for scenario in scenarios:\n",
    "    grouping_col = scenarios[scenario]\n",
    "    for profile in profiles:\n",
    "        profile_filename = f\"profiles_{profiles[profile]}.parquet\"\n",
    "        phenotypic_activity_filename = f\"phenotypic-activity-{profiles[profile]}.csv.gz\"\n",
    "        df = pd.read_parquet(f\"../profiles/{profile_filename}\")\n",
    "        phenotypic_activity_df = (\n",
    "            pd.read_csv(\n",
    "                f\"output/{phenotypic_activity_filename}\",\n",
    "                usecols=[\n",
    "                    f\"{reagent_ID_col}\",\n",
    "                    \"below_corrected_p\",\n",
    "                    \"mean_average_precision\",\n",
    "                ],\n",
    "            )\n",
    "            .rename(\n",
    "                columns={\"mean_average_precision\": \"Metadata_mean_average_precision\"}\n",
    "            )\n",
    "            .query(\"below_corrected_p==True\")\n",
    "            .reset_index(drop=True)\n",
    "            .drop(columns=\"below_corrected_p\")\n",
    "        )\n",
    "        df = df.merge(phenotypic_activity_df, on=f\"{reagent_ID_col}\", how=\"inner\")\n",
    "\n",
    "        for annotation in annotations:\n",
    "            annotation_col = annotations[annotation][\"annotation_col\"]\n",
    "            multi_label_col = annotations[annotation][\"multi_label_col\"]\n",
    "            annotation_df = (\n",
    "                pd.read_csv(\n",
    "                    f\"../00.download-and-process-annotations/output/orf_metadata.tsv.gz\",\n",
    "                    sep=\"\\t\",\n",
    "                    usecols=[\n",
    "                        f\"{reagent_ID_col}\",\n",
    "                        f\"{annotation_col}\",\n",
    "                    ],\n",
    "                )\n",
    "                .dropna()\n",
    "                .assign(col=lambda x: x[f\"{annotation_col}\"].str.split(\"|\"))\n",
    "                .rename(columns={\"col\": f\"{multi_label_col}\"})\n",
    "                .drop(columns=f\"{annotation_col}\")\n",
    "            )\n",
    "\n",
    "            annotated_df = df.merge(annotation_df, on=f\"{reagent_ID_col}\", how=\"inner\")\n",
    "\n",
    "            pos_sameby = [f\"{multi_label_col}\"]\n",
    "            neg_sameby = []\n",
    "            neg_diffby = [f\"{multi_label_col}\"]\n",
    "\n",
    "            if not scenario == \"s3\":\n",
    "                pos_diffby = []\n",
    "            else:\n",
    "                pos_diffby = [f\"{gene_ID_col}\"]\n",
    "\n",
    "            consensus_df = utils.consensus(annotated_df, grouping_col)\n",
    "\n",
    "            if scenario == \"s4\":\n",
    "                consensus_df = consensus_df.sort_values(\n",
    "                    \"Metadata_mean_average_precision\", ascending=False\n",
    "                ).drop_duplicates(subset=[gene_ID_col])\n",
    "            elif scenario == \"s5\":\n",
    "                consensus_df = consensus_df.drop_duplicates(subset=[gene_ID_col])\n",
    "            elif scenario == \"s6\":\n",
    "                consensus_df = consensus_df.sort_values(\n",
    "                    by=[\"Metadata_Insert_Length\", \"Metadata_Prot_Match\"],\n",
    "                    ascending=False,\n",
    "                ).drop_duplicates(subset=[gene_ID_col])\n",
    "\n",
    "            print(\n",
    "                f\"Scenario: {scenario} | Profile: {profile} | Annotation: {annotation}\"\n",
    "            )\n",
    "\n",
    "            metadata_df = utils.get_metadata(consensus_df)\n",
    "            feature_df = utils.get_featuredata(consensus_df)\n",
    "            feature_values = feature_df.values\n",
    "\n",
    "            result = multilabel.average_precision(\n",
    "            metadata_df,\n",
    "            feature_values,\n",
    "            pos_sameby,\n",
    "            pos_diffby,\n",
    "            neg_sameby,\n",
    "            neg_diffby,\n",
    "            batch_size=batch_size,\n",
    "            multilabel_col=multi_label_col,\n",
    "        )\n",
    "\n",
    "            agg_result = mean_average_precision(\n",
    "                result, pos_sameby, null_size, threshold=fdr, seed=12527\n",
    "            )\n",
    "\n",
    "            result[\"scenario\"] = scenario\n",
    "            result[\"annotation\"] = annotation\n",
    "            result[\"profile\"] = profile\n",
    "\n",
    "            agg_result[\"scenario\"] = scenario\n",
    "            agg_result[\"annotation\"] = annotation\n",
    "            agg_result[\"profile\"] = profile\n",
    "\n",
    "            ap_df = pd.concat([ap_df, result], ignore_index=True)\n",
    "            map_df = pd.concat([map_df, agg_result], ignore_index=True)\n",
    "\n",
    "ap_df.to_parquet(f\"output/find-consensus-profiles-ap.parquet\")\n",
    "map_df.to_parquet(f\"output/find-consensus-profiles-map.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
